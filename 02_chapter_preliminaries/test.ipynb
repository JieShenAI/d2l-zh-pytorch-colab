{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(12, dtype=torch.float32).reshape(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12., 15., 18., 21.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(24).reshape(2,3,4).numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  6.,  8., 10.],\n",
       "        [12., 15., 18., 21.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([1,2,3])\n",
    "b = torch.Tensor([2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(9,dtype=torch.float32).reshape(3,3)\n",
    "b = torch.Tensor([0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.],\n",
       "         [6., 7., 8.]]),\n",
       " tensor([0., 1., 0.]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 4., 7.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mv(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.0015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011485"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "0.01*(1-p) + p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13060513713539398"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0015/0.011485"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00176955"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-p)*(0.01*0.03) + p*(0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8307196744935152"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p*(1*0.98)/_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function norm in module torch.functional:\n",
      "\n",
      "norm(input, p='fro', dim=None, keepdim=False, out=None, dtype=None)\n",
      "    Returns the matrix norm or vector norm of a given tensor.\n",
      "    \n",
      "    .. warning::\n",
      "    \n",
      "        torch.norm is deprecated and may be removed in a future PyTorch release.\n",
      "        Its documentation and behavior may be incorrect, and it is no longer\n",
      "        actively maintained.\n",
      "    \n",
      "        Use :func:`torch.linalg.norm`, instead, or :func:`torch.linalg.vector_norm`\n",
      "        when computing vector norms and :func:`torch.linalg.matrix_norm` when\n",
      "        computing matrix norms. Note, however, the signature for these functions\n",
      "        is slightly different than the signature for torch.norm.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): The input tensor. Its data type must be either a floating\n",
      "            point or complex type. For complex inputs, the norm is calculated using the\n",
      "            absolute value of each element. If the input is complex and neither\n",
      "            :attr:`dtype` nor :attr:`out` is specified, the result's data type will\n",
      "            be the corresponding floating point type (e.g. float if :attr:`input` is\n",
      "            complexfloat).\n",
      "    \n",
      "        p (int, float, inf, -inf, 'fro', 'nuc', optional): the order of norm. Default: ``'fro'``\n",
      "            The following norms can be calculated:\n",
      "    \n",
      "            ======  ==============  ==========================\n",
      "            ord     matrix norm     vector norm\n",
      "            ======  ==============  ==========================\n",
      "            'fro'   Frobenius norm  --\n",
      "            'nuc'   nuclear norm    --\n",
      "            Number  --              sum(abs(x)**ord)**(1./ord)\n",
      "            ======  ==============  ==========================\n",
      "    \n",
      "            The vector norm can be calculated across any number of dimensions.\n",
      "            The corresponding dimensions of :attr:`input` are flattened into\n",
      "            one dimension, and the norm is calculated on the flattened\n",
      "            dimension.\n",
      "    \n",
      "            Frobenius norm produces the same result as ``p=2`` in all cases\n",
      "            except when :attr:`dim` is a list of three or more dims, in which\n",
      "            case Frobenius norm throws an error.\n",
      "    \n",
      "            Nuclear norm can only be calculated across exactly two dimensions.\n",
      "    \n",
      "        dim (int, tuple of ints, list of ints, optional):\n",
      "            Specifies which dimension or dimensions of :attr:`input` to\n",
      "            calculate the norm across. If :attr:`dim` is ``None``, the norm will\n",
      "            be calculated across all dimensions of :attr:`input`. If the norm\n",
      "            type indicated by :attr:`p` does not support the specified number of\n",
      "            dimensions, an error will occur.\n",
      "        keepdim (bool, optional): whether the output tensors have :attr:`dim`\n",
      "            retained or not. Ignored if :attr:`dim` = ``None`` and\n",
      "            :attr:`out` = ``None``. Default: ``False``\n",
      "        out (Tensor, optional): the output tensor. Ignored if\n",
      "            :attr:`dim` = ``None`` and :attr:`out` = ``None``.\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of\n",
      "            returned tensor. If specified, the input tensor is casted to\n",
      "            :attr:`dtype` while performing the operation. Default: None.\n",
      "    \n",
      "    .. note::\n",
      "        Even though ``p='fro'`` supports any number of dimensions, the true\n",
      "        mathematical definition of Frobenius norm only applies to tensors with\n",
      "        exactly two dimensions. :func:`torch.linalg.norm` with ``ord='fro'`` aligns\n",
      "        with the mathematical definition, since it can only be applied across\n",
      "        exactly two dimensions.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> import torch\n",
      "        >>> a = torch.arange(9, dtype= torch.float) - 4\n",
      "        >>> b = a.reshape((3, 3))\n",
      "        >>> torch.norm(a)\n",
      "        tensor(7.7460)\n",
      "        >>> torch.norm(b)\n",
      "        tensor(7.7460)\n",
      "        >>> torch.norm(a, float('inf'))\n",
      "        tensor(4.)\n",
      "        >>> torch.norm(b, float('inf'))\n",
      "        tensor(4.)\n",
      "        >>> c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)\n",
      "        >>> torch.norm(c, dim=0)\n",
      "        tensor([1.4142, 2.2361, 5.0000])\n",
      "        >>> torch.norm(c, dim=1)\n",
      "        tensor([3.7417, 4.2426])\n",
      "        >>> torch.norm(c, p=1, dim=1)\n",
      "        tensor([6., 6.])\n",
      "        >>> d = torch.arange(8, dtype= torch.float).reshape(2,2,2)\n",
      "        >>> torch.norm(d, dim=(1,2))\n",
      "        tensor([ 3.7417, 11.2250])\n",
      "        >>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])\n",
      "        (tensor(3.7417), tensor(11.2250))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Built-in mutable sequence.\n",
      "\n",
      "If no argument is given, the constructor creates a new empty list.\n",
      "The argument must be an iterable if specified.\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     _HashedSeq, StackSummary, DeferredConfigList, SList, _ImmutableLineList, FormattedText, NodeList, _ExplodedList, Stack, _Accumulator, ...\n"
     ]
    }
   ],
   "source": [
    "list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Built-in mutable sequence.\n",
      "\n",
      "If no argument is given, the constructor creates a new empty list.\n",
      "The argument must be an iterable if specified.\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     _HashedSeq, StackSummary, DeferredConfigList, SList, _ImmutableLineList, FormattedText, NodeList, _ExplodedList, Stack, _Accumulator, ...\n"
     ]
    }
   ],
   "source": [
    "list??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "randn(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "\n",
      "Returns a tensor filled with random numbers from a normal distribution\n",
      "with mean `0` and variance `1` (also called the standard normal\n",
      "distribution).\n",
      "\n",
      ".. math::\n",
      "    \\text{out}_{i} \\sim \\mathcal{N}(0, 1)\n",
      "\n",
      "The shape of the tensor is defined by the variable argument :attr:`size`.\n",
      "\n",
      "Args:\n",
      "    size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "        Can be a variable number of arguments or a collection like a list or tuple.\n",
      "\n",
      "Keyword args:\n",
      "    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
      "    out (Tensor, optional): the output tensor.\n",
      "    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "        Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "        Default: ``torch.strided``.\n",
      "    device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "        Default: if ``None``, uses the current device for the default tensor type\n",
      "        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "        for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "    requires_grad (bool, optional): If autograd should record operations on the\n",
      "        returned tensor. Default: ``False``.\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> torch.randn(4)\n",
      "    tensor([-2.1436,  0.9966,  2.3426, -0.6366])\n",
      "    >>> torch.randn(2, 3)\n",
      "    tensor([[ 1.5954,  2.8929, -1.0923],\n",
      "            [ 1.1719, -0.4709, -0.1996]])\n",
      "\u001b[1;31mType:\u001b[0m      builtin_function_or_method\n"
     ]
    }
   ],
   "source": [
    "torch.randn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "randn(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "\n",
      "Returns a tensor filled with random numbers from a normal distribution\n",
      "with mean `0` and variance `1` (also called the standard normal\n",
      "distribution).\n",
      "\n",
      ".. math::\n",
      "    \\text{out}_{i} \\sim \\mathcal{N}(0, 1)\n",
      "\n",
      "The shape of the tensor is defined by the variable argument :attr:`size`.\n",
      "\n",
      "Args:\n",
      "    size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "        Can be a variable number of arguments or a collection like a list or tuple.\n",
      "\n",
      "Keyword args:\n",
      "    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
      "    out (Tensor, optional): the output tensor.\n",
      "    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "        Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "        Default: ``torch.strided``.\n",
      "    device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "        Default: if ``None``, uses the current device for the default tensor type\n",
      "        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "        for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "    requires_grad (bool, optional): If autograd should record operations on the\n",
      "        returned tensor. Default: ``False``.\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> torch.randn(4)\n",
      "    tensor([-2.1436,  0.9966,  2.3426, -0.6366])\n",
      "    >>> torch.randn(2, 3)\n",
      "    tensor([[ 1.5954,  2.8929, -1.0923],\n",
      "            [ 1.1719, -0.4709, -0.1996]])\n",
      "\u001b[1;31mType:\u001b[0m      builtin_function_or_method\n"
     ]
    }
   ],
   "source": [
    "torch.randn??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e1e609ed4275b16681bc4abde79d07778f26b9708257cdaa3e43b60c63d82f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
